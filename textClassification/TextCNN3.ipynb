{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(42)\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "from sklearn.metrics import f1_score,accuracy_score\n",
        "\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D,Convolution1D,MaxPooling1D,Conv1D,PReLU,add,Bidirectional\n",
        "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D,GlobalMaxPooling1D,BatchNormalization,CuDNNGRU,CuDNNLSTM\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.callbacks import Callback,EarlyStopping\n",
        "from sklearn.model_selection import train_test_split\n",
        "import warnings\n",
        "warnings.filterwarnings(\u0027ignore\u0027)\n",
        "import keras.backend.tensorflow_backend as KTF\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from imblearn.over_sampling import SMOTE,BorderlineSMOTE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] \u003d \"1\"\n",
        "config \u003d tf.ConfigProto()\n",
        "config.gpu_options.allow_growth\u003dTrue\n",
        "#不全部占满显存, 按需分配\n",
        "sess \u003d tf.Session(config\u003dconfig)\n",
        "KTF.set_session(sess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "#定义超参数\n",
        "MAX_SEQUENCE_LENGTH \u003d 270\n",
        "MAX_NB_WORDS \u003d 20000\n",
        "EMBEDDING_DIM \u003d 300\n",
        "VALIDATION_SPLIT \u003d 0.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\u003cdiv\u003e\n",
              "\u003cstyle scoped\u003e\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "\u003c/style\u003e\n",
              "\u003ctable border\u003d\"1\" class\u003d\"dataframe\"\u003e\n",
              "  \u003cthead\u003e\n",
              "    \u003ctr style\u003d\"text-align: right;\"\u003e\n",
              "      \u003cth\u003e\u003c/th\u003e\n",
              "      \u003cth\u003econtents\u003c/th\u003e\n",
              "      \u003cth\u003euid\u003c/th\u003e\n",
              "      \u003cth\u003equestion\u003c/th\u003e\n",
              "      \u003cth\u003etag_id\u003c/th\u003e\n",
              "      \u003cth\u003etag_name\u003c/th\u003e\n",
              "      \u003cth\u003equestion_answer\u003c/th\u003e\n",
              "      \u003cth\u003equestion_explain\u003c/th\u003e\n",
              "    \u003c/tr\u003e\n",
              "  \u003c/thead\u003e\n",
              "  \u003ctbody\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e0\u003c/th\u003e\n",
              "      \u003ctd\u003e零上 2 记作 2 零下 3 记作 2 A 2 B 3 C 3 DD ∵ 零上 2 记作 2...\u003c/td\u003e\n",
              "      \u003ctd\u003e323b1e899de844eeb55cf247db95694a\u003c/td\u003e\n",
              "      \u003ctd\u003e如果零上2℃记作+2℃那么零下3℃记作+2℃A-2℃B+3℃C-3℃DD∵零上2℃记作+2℃...\u003c/td\u003e\n",
              "      \u003ctd\u003e5091\u003c/td\u003e\n",
              "      \u003ctd\u003e数感\u003c/td\u003e\n",
              "      \u003ctd\u003eD\u003c/td\u003e\n",
              "      \u003ctd\u003e∵零上2℃记作+2℃4零下3℃记作-3℃．故答案为：D\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e1\u003c/th\u003e\n",
              "      \u003ctd\u003e九章算术 中注 ldquo 今 两算 得失 相反 要令 正负 以名 rdquo 意思 今有 ...\u003c/td\u003e\n",
              "      \u003ctd\u003e6e2f76e7c5824e61a7561e6fdd448e71\u003c/td\u003e\n",
              "      \u003ctd\u003e《九章算术》中注有ldquo今两算得失相反要令正负以名之rdquo意思是：今有两数若其意义相...\u003c/td\u003e\n",
              "      \u003ctd\u003e5084\u003c/td\u003e\n",
              "      \u003ctd\u003e运算能力\u003c/td\u003e\n",
              "      \u003ctd\u003eB\u003c/td\u003e\n",
              "      \u003ctd\u003e若气温为零上10℃记作+10℃则-3℃表示气温为零下3℃．故选B\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e2\u003c/th\u003e\n",
              "      \u003ctd\u003e足球 质量 标准 质量 相比 超出 部分 记 作 正数 不足 部分 记 作 负数 下面 4 ...\u003c/td\u003e\n",
              "      \u003ctd\u003e2450bfde6552451893f8a5a01281359b\u003c/td\u003e\n",
              "      \u003ctd\u003e若足球质量与标准质量相比超出部分记作正数不足部分记作负数．则下面4个足球中质量最接近标准的是...\u003c/td\u003e\n",
              "      \u003ctd\u003e5085\u003c/td\u003e\n",
              "      \u003ctd\u003e推理能力\u003c/td\u003e\n",
              "      \u003ctd\u003eC\u003c/td\u003e\n",
              "      \u003ctd\u003e∵|+0.8|\u003d0.8|-3.5|\u003d3.5|-0.7|\u003d0.7|+2.1|\u003d2.10.7lt...\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e3\u003c/th\u003e\n",
              "      \u003ctd\u003e下列 各对量 中 不 具有 相反 意义 胜 3 局 负 3 局 A 收入 3000 元 增加...\u003c/td\u003e\n",
              "      \u003ctd\u003e39e006b41a8a4cf5ba2cd37e1e9bbf1c\u003c/td\u003e\n",
              "      \u003ctd\u003e下列各对量中不具有相反意义的是胜3局与负3局A收入3000元与增加3000元B气温升高4℃与...\u003c/td\u003e\n",
              "      \u003ctd\u003e5085\u003c/td\u003e\n",
              "      \u003ctd\u003e推理能力\u003c/td\u003e\n",
              "      \u003ctd\u003eB\u003c/td\u003e\n",
              "      \u003ctd\u003e收入3000元与支出3000元是相反意义的量故B不符合题意．故答案为：B．\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "    \u003ctr\u003e\n",
              "      \u003cth\u003e4\u003c/th\u003e\n",
              "      \u003ctd\u003e水位 升高 3m 时 水位 变化 记作 3m 水位 下降 3m 时 水位 变化 记 作 3m...\u003c/td\u003e\n",
              "      \u003ctd\u003e1fb8924413c64fc89b5df43d18e1346d\u003c/td\u003e\n",
              "      \u003ctd\u003e如果水位升高3m时水位变化记作+3m那么水位下降3m时水位变化记作-3mA3mB6mC-6m...\u003c/td\u003e\n",
              "      \u003ctd\u003e5085\u003c/td\u003e\n",
              "      \u003ctd\u003e推理能力\u003c/td\u003e\n",
              "      \u003ctd\u003eA\u003c/td\u003e\n",
              "      \u003ctd\u003e因为上升记为+所以下降记为-所以水位下降3m时水位变化记作-3m．故答案为：A．\u003c/td\u003e\n",
              "    \u003c/tr\u003e\n",
              "  \u003c/tbody\u003e\n",
              "\u003c/table\u003e\n",
              "\u003c/div\u003e"
            ],
            "text/plain": [
              "                                            contents  \\\n",
              "0  零上 2 记作 2 零下 3 记作 2 A 2 B 3 C 3 DD ∵ 零上 2 记作 2...   \n",
              "1  九章算术 中注 ldquo 今 两算 得失 相反 要令 正负 以名 rdquo 意思 今有 ...   \n",
              "2  足球 质量 标准 质量 相比 超出 部分 记 作 正数 不足 部分 记 作 负数 下面 4 ...   \n",
              "3  下列 各对量 中 不 具有 相反 意义 胜 3 局 负 3 局 A 收入 3000 元 增加...   \n",
              "4  水位 升高 3m 时 水位 变化 记作 3m 水位 下降 3m 时 水位 变化 记 作 3m...   \n",
              "\n",
              "                                uid  \\\n",
              "0  323b1e899de844eeb55cf247db95694a   \n",
              "1  6e2f76e7c5824e61a7561e6fdd448e71   \n",
              "2  2450bfde6552451893f8a5a01281359b   \n",
              "3  39e006b41a8a4cf5ba2cd37e1e9bbf1c   \n",
              "4  1fb8924413c64fc89b5df43d18e1346d   \n",
              "\n",
              "                                            question  tag_id tag_name  \\\n",
              "0  如果零上2℃记作+2℃那么零下3℃记作+2℃A-2℃B+3℃C-3℃DD∵零上2℃记作+2℃...    5091       数感   \n",
              "1  《九章算术》中注有ldquo今两算得失相反要令正负以名之rdquo意思是：今有两数若其意义相...    5084     运算能力   \n",
              "2  若足球质量与标准质量相比超出部分记作正数不足部分记作负数．则下面4个足球中质量最接近标准的是...    5085     推理能力   \n",
              "3  下列各对量中不具有相反意义的是胜3局与负3局A收入3000元与增加3000元B气温升高4℃与...    5085     推理能力   \n",
              "4  如果水位升高3m时水位变化记作+3m那么水位下降3m时水位变化记作-3mA3mB6mC-6m...    5085     推理能力   \n",
              "\n",
              "  question_answer                                   question_explain  \n",
              "0               D                        ∵零上2℃记作+2℃4零下3℃记作-3℃．故答案为：D  \n",
              "1               B                   若气温为零上10℃记作+10℃则-3℃表示气温为零下3℃．故选B  \n",
              "2               C  ∵|+0.8|\u003d0.8|-3.5|\u003d3.5|-0.7|\u003d0.7|+2.1|\u003d2.10.7lt...  \n",
              "3               B              收入3000元与支出3000元是相反意义的量故B不符合题意．故答案为：B．  \n",
              "4               A           因为上升记为+所以下降记为-所以水位下降3m时水位变化记作-3m．故答案为：A．  "
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\u003d\u003d\n",
            "text/plain": [
              "\u003cFigure size 504x360 with 1 Axes\u003e"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#读取数据，并查看每个类样本数量\n",
        "data \u003d pd.read_csv(\u0027allst_ability.csv\u0027)\n",
        "import matplotlib.pyplot as plt\n",
        "data.groupby(\u0027tag_name\u0027)[\u0027uid\u0027].size().plot(kind\u003d\u0027bar\u0027, figsize\u003d(7, 5))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "#将标签名转换成唯一下标表示\n",
        "le \u003d LabelEncoder()\n",
        "data[\u0027tag_name\u0027] \u003d le.fit_transform(data[\u0027tag_name\u0027])\n",
        "\n",
        "#获取训练数据与样本\n",
        "trainData\u003ddata[\u0027contents\u0027]\n",
        "trainLabel\u003ddata[\u0027tag_name\u0027]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "\u003cclass \u0027pandas.core.series.Series\u0027\u003e\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]\n",
            " [0. 0. 0. ... 0. 0. 1.]]\n"
          ]
        }
      ],
      "source": [
        "num_labels \u003d len(set(trainLabel))\n",
        "print(num_labels)\n",
        "\n",
        "#将样本转换成num_labels维的one-hot表示\n",
        "print(type(trainLabel))\n",
        "trainLabel_onehot \u003d to_categorical(trainLabel.map(lambda x: le.transform([x])[0]), num_labels)\n",
        "print(trainLabel_onehot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (125460, 270)\n"
          ]
        }
      ],
      "source": [
        "#生成字典，num_words:None或整数,个人理解就是对统计单词出现数量后选择次数多的前n个单词，后面的单词都不做处理。\n",
        "tokenizer \u003d text.Tokenizer(num_words\u003dMAX_NB_WORDS,lower\u003dFalse)\n",
        "tokenizer.fit_on_texts(list(trainData))\n",
        "\n",
        "# 使用字典将对应词转成index。shape为 (文档数，每条文档的长度)\n",
        "sequences \u003d tokenizer.texts_to_sequences(trainData)\n",
        "\n",
        "# 将每个文本转成固定长度maxlen，长的截取，短的填充0\n",
        "trainData_pad \u003d sequence.pad_sequences(sequences, maxlen\u003dMAX_SEQUENCE_LENGTH)\n",
        "print(\u0027Shape of data tensor:\u0027, trainData_pad.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "166072\n",
            "(20000, 300)\n"
          ]
        }
      ],
      "source": [
        "# 文本中词的index映射对应的词嵌入\n",
        "word_index \u003d tokenizer.word_index\n",
        "\n",
        "# 基于文本的词典总长为len(word_index)\n",
        "num_words \u003d min(MAX_NB_WORDS, len(word_index))\n",
        "\n",
        "#构建权重矩阵\n",
        "embedding_matrix \u003d np.zeros((num_words, EMBEDDING_DIM))\n",
        "from gensim.models import word2vec\n",
        "model \u003d word2vec.Word2Vec.load(\u0027../word_embedding/word2vec_wx_300\u0027)\n",
        "embedding_matrix \u003d np.zeros((num_words, EMBEDDING_DIM))\n",
        "print(len(word_index.items()))\n",
        "ss\u003d0\n",
        "for word, i in word_index.items():\n",
        "    if word in model.wv.vocab:\n",
        "        ss +\u003d 1\n",
        "        if i \u003e\u003d num_words:\n",
        "            break\n",
        "        embedding_matrix[i] \u003d model.wv[word]\n",
        "    else:\n",
        "        pass\n",
        "print(embedding_matrix.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u003cclass \u0027numpy.ndarray\u0027\u003e \u003cclass \u0027numpy.ndarray\u0027\u003e \u003cclass \u0027numpy.ndarray\u0027\u003e \u003cclass \u0027numpy.ndarray\u0027\u003e\n",
            "(6273, 10)\n"
          ]
        }
      ],
      "source": [
        "#划分训练集与测试集\n",
        "X_train, X_test, Y_train, Y_test \u003d train_test_split(trainData_pad,trainLabel_onehot,test_size\u003d0.05, random_state\u003d3619)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# TextCNN模型\n",
        "def get_model():    \n",
        "    inp \u003d Input(shape\u003d(MAX_SEQUENCE_LENGTH, ))\n",
        "    x \u003d Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights\u003d[embedding_matrix])(inp)\n",
        "    x \u003d SpatialDropout1D(0.4)(x)\n",
        "    x \u003d Reshape((MAX_SEQUENCE_LENGTH, EMBEDDING_DIM, 1))(x)\n",
        "    \n",
        "    conv_0 \u003d Conv2D(num_filters, kernel_size\u003d(filter_sizes[0], EMBEDDING_DIM), kernel_initializer\u003d\u0027normal\u0027,\n",
        "                                                                                    activation\u003d\u0027elu\u0027)(x)\n",
        "    conv_1 \u003d Conv2D(num_filters, kernel_size\u003d(filter_sizes[1], EMBEDDING_DIM), kernel_initializer\u003d\u0027normal\u0027,\n",
        "                                                                                    activation\u003d\u0027elu\u0027)(x)\n",
        "    conv_2 \u003d Conv2D(num_filters, kernel_size\u003d(filter_sizes[2], EMBEDDING_DIM), kernel_initializer\u003d\u0027normal\u0027,\n",
        "                                                                                    activation\u003d\u0027elu\u0027)(x)\n",
        "    conv_3 \u003d Conv2D(num_filters, kernel_size\u003d(filter_sizes[3], EMBEDDING_DIM), kernel_initializer\u003d\u0027normal\u0027,\n",
        "                                                                                    activation\u003d\u0027elu\u0027)(x)\n",
        "    \n",
        "    maxpool_0 \u003d MaxPool2D(pool_size\u003d(maxlen - filter_sizes[0] + 1, 1))(conv_0)\n",
        "    maxpool_1 \u003d MaxPool2D(pool_size\u003d(maxlen - filter_sizes[1] + 1, 1))(conv_1)\n",
        "    maxpool_2 \u003d MaxPool2D(pool_size\u003d(maxlen - filter_sizes[2] + 1, 1))(conv_2)\n",
        "    maxpool_3 \u003d MaxPool2D(pool_size\u003d(maxlen - filter_sizes[3] + 1, 1))(conv_3)\n",
        "        \n",
        "    z \u003d Concatenate(axis\u003d1)([maxpool_0, maxpool_1, maxpool_2, maxpool_3])   \n",
        "    z \u003d Flatten()(z)\n",
        "    z \u003d Dropout(0.1)(z)\n",
        "        \n",
        "    outp \u003d Dense(10, activation\u003d\"sigmoid\")(z)\n",
        "    \n",
        "    model \u003d Model(inputs\u003dinp, outputs\u003doutp)\n",
        "    model.compile(loss\u003d\u0027binary_crossentropy\u0027,\n",
        "                  optimizer\u003d\u0027adam\u0027,\n",
        "                  metrics\u003d[\u0027accuracy\u0027])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": "# CONv + GRU模型\n\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D,GlobalAveragePooling1D,concatenate\n\nfile_path \u003d \"best_model.hdf5\"\n\ncheck_point \u003d ModelCheckpoint(file_path, monitor \u003d \"val_loss\", verbose \u003d 1,save_best_only \u003d True, mode \u003d \"min\")\nearly_stop \u003d EarlyStopping(monitor \u003d \"val_loss\", mode \u003d \"min\", patience \u003d 5)\n\n#激活函数 sigmoid多用于二分类，softmax多用于多分类\n#损失函数 binary_crossentropy多用于二分类，categorical_crossentropy多用于多分类\n\ndef build_model(units\u003d128):\n    inp \u003d Input(shape \u003d (MAX_SEQUENCE_LENGTH,))\n    x \u003d Embedding(MAX_NB_WORDS, EMBEDDING_DIM, weights \u003d [embedding_matrix], trainable \u003d False)(inp)\n    x1 \u003d SpatialDropout1D(0.3)(x)\n\n    x \u003d Bidirectional(CuDNNGRU(units, return_sequences \u003d True))(x1)\n    x \u003d Conv1D(64, kernel_size \u003d 2, padding \u003d \"valid\", kernel_initializer \u003d \"he_uniform\")(x)\n    y \u003d Bidirectional(CuDNNLSTM(units, return_sequences \u003d True))(x1)\n    y \u003d Conv1D(64, kernel_size \u003d 2, padding \u003d \"valid\", kernel_initializer \u003d \"he_uniform\")(y)\n    \n    avg_pool1 \u003d GlobalAveragePooling1D()(x)\n    max_pool1 \u003d GlobalMaxPooling1D()(x)\n\n    avg_pool2 \u003d GlobalAveragePooling1D()(y)\n    max_pool2 \u003d GlobalMaxPooling1D()(y)\n\n    x \u003d concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n    x \u003d Dense(10, activation \u003d \"softmax\")(x)\n\n    model \u003d Model(inputs \u003d inp, outputs \u003d x)\n    model.compile(loss \u003d \"binary_crossentropy\", optimizer\u003d\u0027adam\u0027, metrics \u003d [\"accuracy\"])\n    return model"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# LSTM 模型\n",
        "from keras.layes import LSTM\n",
        "from keras.models import Sequential,Model\n",
        "\n",
        "def build_LSTM_model():\n",
        "    model\u003dSequential()\n",
        "    model.add(Embedding(MAX_NB_WORDS,EMBEDDING_DIM,weights\u003d[embedding_matrix],input_length\u003dMAX_SEQUENCE_LENGTH,trainable\u003dFlase))\n",
        "    model.add(LSTM(EMBEDDING_DIM,dropout\u003d0.2,recurrent_dropout\u003d0.2))\n",
        "    model.add(Dense(num_labels,activation\u003d\u0027softmax\u0027))\n",
        "    model.compile(loss\u003d\u0027categorical_crossentropy\u0027,optimizer\u003d\u0027adam\u0027,metrics\u003d[\u0027acc\u0027])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "#LSTM+Attention模型\n",
        "from keras.engine.topology import Layer\n",
        "def build_attentionLSTM_model():\n",
        "    embedding_layer \u003d Embedding(MAX_NB_WORDS,\n",
        "                                EMBEDDING_DIM,\n",
        "                                weights\u003d[embedding_matrix],\n",
        "                                input_length\u003dMAX_SEQUENCE_LENGTH,\n",
        "                                trainable\u003dFalse)\n",
        "    sequence_input \u003d Input(shape\u003d(MAX_SEQUENCE_LENGTH,), dtype\u003d\u0027int32\u0027)\n",
        "    embedded_sequences \u003d embedding_layer(sequence_input)\n",
        "\n",
        "    l_lstm \u003d Bidirectional(LSTM(EMBEDDING_DIM, return_sequences\u003dTrue))(embedded_sequences)\n",
        "    l_att \u003d AttLayer()(l_lstm)\n",
        "    preds \u003d Dense(num_labels, activation\u003d\u0027softmax\u0027)(l_att)\n",
        "    model \u003d Model(sequence_input, preds)\n",
        "\n",
        "    model.compile(loss\u003d\u0027categorical_crossentropy\u0027,\n",
        "                  optimizer\u003d\u0027adam\u0027,\n",
        "                  metrics\u003d[\u0027acc\u0027])\n",
        "    \n",
        "# Attention GRU network\n",
        "class AttLayer(Layer):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.init \u003d initializers.get(\u0027normal\u0027)\n",
        "        #self.input_spec \u003d [InputSpec(ndim\u003d3)]\n",
        "        super(AttLayer, self).__init__(**kwargs)\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape)\u003d\u003d3\n",
        "        #self.W \u003d self.init((input_shape[-1],1))\n",
        "        self.W \u003d self.init((input_shape[-1],))\n",
        "        #self.input_spec \u003d [InputSpec(shape\u003dinput_shape)]\n",
        "        self.trainable_weights \u003d [self.W]\n",
        "        super(AttLayer, self).build(input_shape) # be sure you call this somewhere!\n",
        "    def call(self, x, mask\u003dNone):\n",
        "        eij \u003d K.tanh(K.dot(x, self.W))\n",
        "        ai \u003d K.exp(eij)\n",
        "        weights \u003d ai/K.sum(ai, axis\u003d1).dimshuffle(0,\u0027x\u0027)\n",
        "        weighted_input \u003d x*weights.dimshuffle(0,1,\u0027x\u0027)\n",
        "        return weighted_input.sum(axis\u003d1)\n",
        "\n",
        "    def get_output_shape_for(self, input_shape):\n",
        "        return (input_shape[0], input_shape[-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "#LSTM+Attention 2 模型\n",
        "from keras import backend as K\n",
        "from keras.engine.topology import Layer\n",
        "#from keras import initializations\n",
        "from keras import initializers, regularizers, constraints\n",
        "class Attention(Layer):\n",
        "    def __init__(self, step_dim,\n",
        "                 W_regularizer\u003dNone, b_regularizer\u003dNone,\n",
        "                 W_constraint\u003dNone, b_constraint\u003dNone,\n",
        "                 bias\u003dTrue, **kwargs):\n",
        "        \"\"\"\n",
        "        Keras Layer that implements an Attention mechanism for temporal data.\n",
        "        Supports Masking.\n",
        "        Follows the work of Raffel et al. [https://arxiv.org/abs/1512.08756]\n",
        "        # Input shape\n",
        "            3D tensor with shape: `(samples, steps, features)`.\n",
        "        # Output shape\n",
        "            2D tensor with shape: `(samples, features)`.\n",
        "        :param kwargs:\n",
        "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences\u003dTrue.\n",
        "        The dimensions are inferred based on the output shape of the RNN.\n",
        "        Example:\n",
        "            model.add(LSTM(64, return_sequences\u003dTrue))\n",
        "            model.add(Attention())\n",
        "        \"\"\"\n",
        "        self.supports_masking \u003d True\n",
        "        #self.init \u003d initializations.get(\u0027glorot_uniform\u0027)\n",
        "        self.init \u003d initializers.get(\u0027glorot_uniform\u0027)\n",
        "\n",
        "        #regularizers:正则化器允许在优化过程中对层的参数或层的激活情况进行惩罚。 \n",
        "        \n",
        "        self.W_regularizer \u003d regularizers.get(W_regularizer)\n",
        "        self.b_regularizer \u003d regularizers.get(b_regularizer)\n",
        "\n",
        "        #constraints 模块的函数允许在优化期间对网络参数设置约束（例如非负性）,约束是以层为对象进行的。约束层开放 2 个关键字参数：\n",
        "        #kernel_constraint 用于主权重矩阵。\n",
        "        #bias_constraint 用于偏置。\n",
        "\n",
        "        self.W_constraint \u003d constraints.get(W_constraint)\n",
        "        self.b_constraint \u003d constraints.get(b_constraint)\n",
        "\n",
        "        self.bias \u003d bias\n",
        "        self.step_dim \u003d step_dim\n",
        "        self.features_dim \u003d 0\n",
        "        super(Attention, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert len(input_shape) \u003d\u003d 3  #断言，如果input_shape长度不为3就报错\n",
        "        #输入形状：（samples:样本数 批量处理的数目 ， step_dim:样本长度 限定文本的最大长度 ，embedding_dim:词向量长度）\n",
        "        self.W \u003d self.add_weight((input_shape[-1],),\n",
        "                                 initializer\u003dself.init,\n",
        "                                 name\u003d\u0027{}_W\u0027.format(self.name),\n",
        "                                 regularizer\u003dself.W_regularizer,\n",
        "                                 constraint\u003dself.W_constraint)\n",
        "        self.features_dim \u003d input_shape[-1]\n",
        "\n",
        "        if self.bias:\n",
        "            self.b \u003d self.add_weight((input_shape[1],),\n",
        "                                     initializer\u003d\u0027zero\u0027,\n",
        "                                     name\u003d\u0027{}_b\u0027.format(self.name),\n",
        "                                     regularizer\u003dself.b_regularizer,\n",
        "                                     constraint\u003dself.b_constraint)\n",
        "        else:\n",
        "            self.b \u003d None\n",
        "\n",
        "        self.built \u003d True\n",
        "\n",
        "    def compute_mask(self, input, input_mask\u003dNone):\n",
        "        # do not pass the mask to the next layers\n",
        "        return None\n",
        "\n",
        "    def call(self, x, mask\u003dNone):\n",
        "        # eij \u003d K.dot(x, self.W) TF backend doesn\u0027t support it\n",
        "\n",
        "        # features_dim \u003d self.W.shape[0]\n",
        "        # step_dim \u003d x._keras_shape[1]\n",
        "\n",
        "        features_dim \u003d self.features_dim\n",
        "        step_dim \u003d self.step_dim\n",
        "        \n",
        "        eij \u003d K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
        "        if self.bias:\n",
        "            eij +\u003d self.b\n",
        "        eij \u003d K.tanh(eij)\n",
        "        a \u003d K.exp(eij)\n",
        "\n",
        "        # apply mask after the exp. will be re-normalized next\n",
        "        if mask is not None:\n",
        "            # Cast the mask to floatX to avoid float64 upcasting in theano ; \n",
        "            #cast:将张量转换到不同的 dtype 并返回。\n",
        "            a *\u003d K.cast(mask, K.floatx())\n",
        "\n",
        "        # in some cases especially in the early stages of training the sum may be almost zero\n",
        "        #epsilon() 返回一个很小的浮点数，以防止除0错误\n",
        "        a /\u003d K.cast(K.sum(a, axis\u003d1, keepdims\u003dTrue) + K.epsilon(), K.floatx())\n",
        "\n",
        "        #expand_dims,在dim的轴上增加一维，dim默认为-1  [1,2,3]-\u003e[[1]\n",
        "#                                                               [2]\n",
        "#                                                               [3]]\n",
        "        a \u003d K.expand_dims(a)\n",
        "        weighted_input \u003d x * a\n",
        "        #print weigthted_input.shape\n",
        "        #列方向求和\n",
        "        return K.sum(weighted_input, axis\u003d1)\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        #return input_shape[0], input_shape[-1]\n",
        "        #返回：（样本数(sample)，样本长度（step_dim））\n",
        "        return input_shape[0],  self.features_dim\n",
        "def getModel():\n",
        "    rate_drop_lstm \u003d 0.25\n",
        "    rate_drop_dense \u003d 0.25\n",
        "\n",
        "    embedding_layer \u003d Embedding(MAX_NB_WORDS,\n",
        "        EMBEDDING_DIM,\n",
        "        weights\u003d[embedding_matrix],\n",
        "        input_length\u003dMAX_SEQUENCE_LENGTH,\n",
        "        trainable\u003dFalse)\n",
        "\n",
        "    lstm_layer \u003d LSTM(EMBEDDING_DIM, dropout\u003drate_drop_lstm, recurrent_dropout\u003drate_drop_lstm,return_sequences\u003dTrue)\n",
        "\n",
        "    comment_input \u003d Input(shape\u003d(MAX_SEQUENCE_LENGTH,), dtype\u003d\u0027int32\u0027)\n",
        "    embedded_sequences\u003d embedding_layer(comment_input)\n",
        "    x \u003d lstm_layer(embedded_sequences)\n",
        "    x \u003d Dropout(rate_drop_dense)(x)\n",
        "    merged \u003d Attention(MAX_SEQUENCE_LENGTH)(x)\n",
        "    merged \u003d Dense(num_dense, activation\u003dact)(merged)\n",
        "    merged \u003d Dropout(rate_drop_dense)(merged)\n",
        "   # BatchNormalization 批量标准化层 即，应用一个维持激活项平均值接近 0，标准差接近 1 的转换。\n",
        "    merged \u003d BatchNormalization()(merged)\n",
        "    preds \u003d Dense(num_labels, activation\u003d\u0027sigmoid\u0027)(merged)\n",
        "\n",
        "    ########################################\n",
        "    ## train the model\n",
        "    ########################################\n",
        "    model \u003d Model(inputs\u003d[comment_input], outputs\u003dpreds)\n",
        "    model.compile(loss\u003d\u0027categorical_crossentropy\u0027,\n",
        "                  optimizer\u003d\u0027adam\u0027,\n",
        "                  metrics\u003d[\u0027acc\u0027])\n",
        "    print(model.summary())\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "# 定义训练参数\n",
        "filter_sizes \u003d [1,2,3,5]\n",
        "num_filters \u003d 32\n",
        "batch_size \u003d 256\n",
        "epochs \u003d 10\n",
        "# model\u003dget_model()\n",
        "\n",
        "model \u003d build_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "119187 119187\n"
          ]
        }
      ],
      "source": [
        "#自定义每个类别权重\n",
        "class_dict \u003d {9:1,3:3,2:9,4:21,1:22,0:26,6:50,5:60,7:80,8:150}\n",
        "\n",
        "## 建立字典，设置不同的采样权重\n",
        "resample_dict \u003d {8:800,7:1200,5:1600,6:1800,0:3400,1:4000,4:4242}\n",
        "\n",
        "# 进行采样\n",
        "bsm \u003d BorderlineSMOTE(sampling_strategy\u003dresample_dict, random_state\u003d42)\n",
        "# bsm \u003d BorderlineSMOTE(random_state\u003d42)\n",
        "#X_res, y_res \u003d x_train, y_train\n",
        "print(len(X_train),len(Y_train))\n",
        "X_res, y_res \u003d bsm.fit_resample(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 121523 samples, validate on 6396 samples\n",
            "Epoch 1/10\n",
            " - 54s - loss: 0.7447 - acc: 0.9128 - val_loss: 0.2306 - val_acc: 0.9048\n",
            "Epoch 2/10\n",
            " - 54s - loss: 0.7439 - acc: 0.9131 - val_loss: 0.2404 - val_acc: 0.9015\n",
            "Epoch 3/10\n",
            " - 54s - loss: 0.7307 - acc: 0.9127 - val_loss: 0.2308 - val_acc: 0.9050\n",
            "Epoch 4/10\n",
            " - 54s - loss: 0.7250 - acc: 0.9130 - val_loss: 0.2463 - val_acc: 0.9007\n",
            "Epoch 5/10\n",
            " - 54s - loss: 0.7193 - acc: 0.9133 - val_loss: 0.2359 - val_acc: 0.9046\n",
            "Epoch 6/10\n",
            " - 54s - loss: 0.7094 - acc: 0.9135 - val_loss: 0.2265 - val_acc: 0.9087\n",
            "Epoch 7/10\n",
            " - 54s - loss: 0.6925 - acc: 0.9141 - val_loss: 0.2444 - val_acc: 0.9039\n",
            "Epoch 8/10\n",
            " - 54s - loss: 0.6929 - acc: 0.9141 - val_loss: 0.2610 - val_acc: 0.8966\n",
            "Epoch 9/10\n",
            " - 54s - loss: 0.6834 - acc: 0.9147 - val_loss: 0.2378 - val_acc: 0.9071\n",
            "Epoch 10/10\n",
            " - 54s - loss: 0.6784 - acc: 0.9144 - val_loss: 0.2376 - val_acc: 0.9054\n"
          ]
        }
      ],
      "source": [
        "#划分训练与验证集，random_state可调\n",
        "X_tra, X_val, y_tra, y_val \u003d train_test_split(X_res,y_res, train_size\u003d0.95, random_state\u003d2018)\n",
        "\n",
        "#训练\n",
        "hist \u003d model.fit(X_tra, y_tra, batch_size\u003dbatch_size, epochs\u003depochs,class_weight\u003dclass_dict, validation_data\u003d(X_val, y_val), verbose\u003d2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.5260640841702535"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#预测\n",
        "y_pred \u003d model.predict(X_test, batch_size\u003dbatch_size)\n",
        "\n",
        "#调整结果格式\n",
        "top \u003d np.argsort(-model.predict(X_test), axis\u003d1)[:, :1]\n",
        "result\u003d[]\n",
        "for r in top:\n",
        "    result.append(r[0])\n",
        "Y_test \u003d np.argmax(Y_test, axis\u003d1)   \n",
        "Y_test\u003dnp.asarray(Y_test)\n",
        "result\u003dnp.asarray(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "#正确率\n",
        "# acc \u003d accuracy_score(Y_test,np.argmax(y_pred, axis\u003d1))\n",
        "acc \u003d accuracy_score(Y_test,result)\n",
        "acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.592977871621184"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#f1值\n",
        "score_cv \u003d f1_score(Y_test, result, labels\u003drange(0, 10), average\u003d\u0027weighted\u0027)\n",
        "score_cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}